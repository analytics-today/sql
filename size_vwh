-- 
-- Size Virtual Warehouse
--
-- This query returns for every Virtual Warehouse
--   o The current size of the warehouse (XSMALL to X4LARGE)
--   o The percentage of large queries (over 1Gb scanned)
--   o Percentage of small queries (under 1Gb scanned)
--   o The average size of large queries (Gb's returned by queries over 1Gb in size)
--   o The average query time (seconds) for large queries
--   o Total count of queries
--
-- The purpose is to provide an indication of the workload by warehouse
--
-- It should be used to detect:
--   o Large queries (>1Gb) being executed on relatively small (XSMALL, SMALL or MEDIUM) warehouses
--   o Small queries (<1Gb) being executed on relatively large (LARGE, XLARGE to X4LARGE) warehouses
--
select warehouse_name
,      warehouse_size 
,     round(count_large / count_queries * 100,0) as percent_large
,     round(count_small / count_queries * 100,0) as percent_small
,     case
           when avg_large >= power(2, 40) then to_char(round(avg_large / power(2, 40), 1)) || ' TB'
           when avg_large >= power(2, 30) then to_char(round(avg_large / power(2, 30), 1)) || ' GB'
           when avg_large >= power(2, 20) then to_char(round(avg_large / power(2, 20), 1)) || ' MB'
           when avg_large >= power(2, 10) then to_char(round(avg_large / power(2, 10), 1)) || ' K'
           else to_char(avg_large)
      end as avg_bytes_large
,     round(avg_large_exe_time) as avg_large_exe_time
,     round(avg_execution_time) as avg_all_exe_time
,     count_queries
from (
    select 
        warehouse_name
    ,   warehouse_size
    ,   avg(case
           when bytes_scanned >= power(2, 30) then bytes_scanned
           else null
        end) as avg_large 
    ,   count(case
           when bytes_scanned >= power(2, 30) then 1
           else null
        end) as count_large   
    ,   count(case
           when bytes_scanned < power(2, 30) then 1
           else null
        end) as count_small    
    ,   avg(case
            when bytes_scanned >= power(2,30) then total_elapsed_time/1000
            else null
        end) as avg_large_exe_time
    ,   avg(bytes_scanned)              as avg_bytes_scanned
    ,   avg(total_elapsed_time)/1000    as avg_elapsed_time
    ,   avg(execution_time)/1000        as avg_execution_time
    ,   count(*)                        as count_queries
    from snowflake.account_usage.query_history q 
    where execution_status = 'SUCCESS'
    and warehouse_size is not null
    and start_time > dateadd(month, -1, current_date())
    group by warehouse_name
    ,        warehouse_size)
order by warehouse_name;

